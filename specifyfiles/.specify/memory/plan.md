# Implementation Plan: Online Book Foundational Structure

**Feature Branch**: `feat/book-structure-initial`
**Date**: 2025-12-06
**Spec**: `specifyfiles/.specify/memory/spec.md`

## 1. Summary

This plan outlines the technical steps required to create the foundational file and directory structure for the online book, "The Guide to Modern Agentic Development". The implementation will scaffold the main content pages and the complete 13-week modular structure within the Docusaurus `docs` directory. The approach is designed to leverage Docusaurus's autogenerated sidebars for maintainability, directly fulfilling the requirements of the project specification and constitution.

## 2. Constitution Check

This plan strictly adheres to the principles defined in `specifyfiles/.specify/memory/constitution.md`:

*   ✅ **Technical Platform Principle**: All custom components will use TypeScript. The plan focuses on Markdown files, which is platform-agnostic.
*   ✅ **Context7 Principle**: All subsequent automated actions to populate content will adhere to this principle.
*   ✅ **Content Principle**: The core of this plan is to create the 13-week modular structure mandated by the constitution.
*   ✅ **Hardware Principle**: The plan includes the creation of a dedicated file for the "Hardware Requirements".

## 3. Project Structure and Implementation

The following file and directory structure will be created within the `docs` directory.

### 3.1. Top-Level Content Pages

These files will serve as the primary informational pages for the book.

```text
docs/
├── learning-outcomes.md
├── hardware-requirements.md
└── assessments.md
```

### 3.2. Weekly Module Structure

A directory will be created for each of the 13 weeks. This modular structure directly maps to the course syllabus and the content principle of the constitution.

```text
docs/
├── week-1/
│   ├── _category_.json
│   └── introduction.md
├── week-2/
│   ├── _category_.json
│   └── introduction.md
├── week-3/
│   ├── _category_.json
│   └── introduction.md
├── week-4/
│   ├── _category_.json
│   └── introduction.md
├── week-5/
│   ├── _category_.json
│   └── introduction.md
├── week-6/
│   ├── _category_.json
│   └── introduction.md
├── week-7/
│   ├── _category_.json
│   └── introduction.md
├── week-8/
│   ├── _category_.json
│   └── introduction.md
├── week-9/
│   ├── _category_.json
│   └── introduction.md
├── week-10/
│   ├── _category_.json
│   └── introduction.md
├── week-11/
│   ├── _category_.json
│   └── introduction.md
├── week-12/
│   ├── _category_.json
│   └── introduction.md
└── week-13/
    ├── _category_.json
    └── introduction.md
```

### 3.3. Category Configuration

Each `_category_.json` file will contain JSON to define the sidebar label for its respective week, ensuring a clean and ordered navigation experience. For example, `docs/week-1/_category_.json` will contain:

```json
{
  "label": "Week 1 - Introduction to AI and Robotics",
  "position": 1
}
```
*(Note: Labels for weeks 2-13 will be populated with their respective topics during implementation.)*

## 4. Sidebar Strategy

The `sidebars.ts` file is currently configured for autogeneration (`tutorialSidebar: [{type: 'autogenerated', dirName: '.'}]`). This plan will **leverage this feature**. By creating the directory structure outlined above with corresponding `_category_.json` files, the sidebar will be generated automatically.

**This approach requires NO manual edits to `sidebars.ts`** and ensures that the sidebar remains in sync with the file system, making it scalable and easy to maintain.

## 5. High-Level Implementation Steps

1.  **Create Top-Level Pages**: Create the empty markdown files: `learning-outcomes.md`, `hardware-requirements.md`, and `assessments.md` in the `docs/` directory.
2.  **Scaffold Weekly Modules**: Programmatically create the directory structure for `week-1` through `week-13`.
3.  **Populate Categories**: In each weekly directory, create a `_category_.json` file with the appropriate `label` and `position`.
4.  **Create Placeholder Content**: In each weekly directory, create a placeholder `introduction.md` file to ensure the category is not empty.
5.  **Verification**: Run the Docusaurus development server to confirm that all new pages and 13 weekly modules appear correctly in the autogenerated sidebar.

## RAG Chatbot Implementation Plan

**Feature Branch**: `feat/rag-chatbot-opensource`
**Spec**: `specifyfiles/.specify/memory/spec.md#RAG-Chatbot-Integration-Feature`

### 1. Summary
This plan details the technical implementation for a RAG (Retrieval-Augmented Generation) chatbot based on an **Open-Source/Free Tier RAG Stack**. The chatbot will answer user questions based on the book's content, leveraging a local vectorization pipeline, a Hugging Face-powered FastAPI backend, and a custom React frontend component.

### 2. Constitution Check
*   ✅ **Technical Platform Principle**: All custom React components will use TypeScript. The backend will use Python with FastAPI.
*   ✅ **Context7 Principle**: The vectorization pipeline will incorporate metadata to handle versioned documentation. The RAG prompt will reference the `Context7` framework.
*   ✅ **Content Principle**: The chatbot's knowledge base will be derived directly from the approved weekly modules and content pages.
*   ✅ **Hardware Principle**: Not applicable.

### 3. Stage 1: Vectorization Pipeline (Local Embeddings)
This stage focuses on creating a script (`scripts/vectorize.py`) to process and upload the book's content to the Qdrant vector store using a local sentence transformer.

**Implementation Steps**:
1.  **Install Dependencies**: Add `qdrant-client`, `beautifulsoup4`, `markdown-it-py`, and `sentence-transformers` to a `requirements.txt` for the pipeline.
2.  **Content Scraper**:
    *   Implement a function to scan the `docs/` directory for all `.md` and `.mdx` files.
    *   Parse the content into clean text, stripping front matter and JSX.
3.  **Text Chunking**:
    *   Implement a text splitter to break the text into smaller, semantically coherent chunks (e.g., 500-1000 characters with overlap).
    *   Attach metadata to each chunk (source file path, headings).
4.  **Embedding Generation (Local)**:
    *   Load a local Sentence Transformer model (e.g., `all-MiniLM-L6-v2`) from the Hugging Face library.
    *   For each text chunk, generate a vector using the loaded model.
5.  **Qdrant Vector Upload**:
    *   Initialize the Qdrant client.
    *   Create a Qdrant collection named `book_content_v1` if it doesn't exist, configuring it for the chosen model's vector size (e.g., 384 for `all-MiniLM-L6-v2`).
    *   Batch-upload the generated vectors and their metadata to the collection.

### 4. Stage 2: FastAPI Backend Service (Hugging Face RAG Pipeline)
This stage focuses on creating the FastAPI backend to serve the full RAG pipeline.

**Implementation Steps**:
1.  **Server Setup**:
    *   Create `api/main.py` with a basic FastAPI application.
    *   Define a `requirements.txt` with `fastapi`, `uvicorn`, `qdrant-client`, `pydantic`, `sentence-transformers`, and `huggingface_hub`.
2.  **RAG Chat Endpoint**:
    *   Define a POST endpoint `/api/rag/chat`.
    *   This endpoint will receive a JSON payload with the user's query (`query: string`) and optional selected context (`context: string`).
3.  **RAG Pipeline Logic**:
    *   **Embedding**: Load the same local Sentence Transformer model (`all-MiniLM-L6-v2`) to generate an embedding for the incoming query.
    *   **Retrieval**: Query the Qdrant `book_content_v1` collection to find the most relevant text chunks based on the query embedding.
    *   **Prompt Construction**: Construct a detailed prompt for the LLM, combining the user's query, the retrieved context chunks, and instructions to answer based only on the provided sources.
    *   **LLM Call**: Use the `huggingface_hub` library to call a free-tier model on the Hugging Face Inference API (e.g., a Llama 3 model). Pass the constructed prompt.
    *   **Response**: Return a JSON response containing the generated answer and the source metadata from the retrieved chunks.

### 5. Stage 3: Docusaurus Frontend Component (Custom React UI)
This stage focuses on integrating a custom chat UI into the Docusaurus site.

**Implementation Steps**:
1.  **Component Scaffolding**:
    *   Create a new React component at `src/components/Chatbot/index.tsx`.
2.  **Custom Chat UI**:
    *   Build the chat interface (chat window, message list, input field) using standard React components.
    *   Manage the conversation state (messages, loading status, errors) within the component.
    *   On submit, make a `fetch` request to the FastAPI endpoint (`/api/rag/chat`) with the user's query.
    *   Render the response from the backend, including the answer and source links.
    *   Style the component to match the Docusaurus theme, floating it in the bottom-right corner.
3.  **Context Capture Logic**:
    *   Implement a global event listener (e.g., on `mouseup`) to detect text selection in the main content area.
    *   If text is selected, display a button (e.g., "Ask about this") that, when clicked, passes the selected text as context to the chatbot.
4.  **Site Integration**:
    *   Import and render the `<Chatbot />` component in a global layout file, such as `src/theme/Root.tsx`, to ensure it is available on all pages.
